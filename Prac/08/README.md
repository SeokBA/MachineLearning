# 08. 딥 러닝(Deep Learning) 개요
#### 참고 : https://wikidocs.net/22882

계단함수가 아닌 시그모이드 함수를 사용하는 이유 : 역전파를 계산할 때 미분을 수행해야하는데 계단함수는 거의 모든 구간에서 기울기가 0이기 때문에 학습이 제대로 되지않음

순전파 => 예측값을 구함
역전파 => (예측값을 구한 뒤) 예측값과 실제값으로부터 오차를 계산하고 오차로부터 가중치와 편향 업데이트 (순전파의 반대방향으로 업데이트)


옵티마이저
    1. 배치 경사 하강법 : 모든 데이터
    2. 확률적 경사 하강법 : 1개의 데이터
    3. 미니 배치 경사 하강법 : 일부 데이터
    4. 모멘텀 : SGD에서 계산된 접선의 기울기에 한 시점(step) 전의 접선의 기울기값을 일정한 비율만큼 반영
    5. Adagrad : 각 매개변수에 서로다른 학습률 적용
    6. RMSprop : 학습 지속시 학습률이 떨어지는 것을 막기위해 다른 수식으로 대체
    7. Adam : Adagrad + RMSprop